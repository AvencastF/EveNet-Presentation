---
class: py-10
---

# To bring them all...

<span>Summary of EveNet achievements</span>

<v-clicks>

- Trained <span class="gradient-animated" style="font-variant: small-caps;">EveNet</span> encoder-decoder model with 5 task-specific heads on <span text="[#00e5ff]">500M fast-simulated data</span>
- Tested <span class="gradient-animated" style="font-variant: small-caps;">EveNet</span> performance across <span text="[#00e5ff]">4 downstream tasks</span>
  - Different detectors and kinematics regimes
  - Pile-up simulated samples and <span text="[#00e5ff]">real data</span>
- Achieved superior performance with <span text="[#00e5ff]">2-stage pretraining</span>
  - Outperforms scratch model and task-specific models
  - Including <span text="[#00e5ff]">tabular foundation models</span>
- Simplified workflow for typical S/B analysis
  - With pretraining, <span class="gradient-animated" style="font-variant: small-caps;">EveNet</span> eliminates need for multiple aux heads
  - <span text="[#00e5ff]">Classification head alone</span> is sufficient
- Demonstrated <span text="[#00e5ff]">robustness to systematic variations</span>
- Achieved <span text="[#00e5ff]">fast convergence</span> for full model
  - Up to <span text="[#00e5ff]">3Ã— faster</span> than scratch training
- Works effectively in <span text="[#00e5ff]">low-statistics regimes</span>
  - Both in <span text="[#00e5ff]">classification</span> and <span text="[#00e5ff]">generative tasks</span>

</v-clicks>
